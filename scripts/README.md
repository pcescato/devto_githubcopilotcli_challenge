# Scripts Directory

Utilities for database migration, automated sync workers, and maintenance.

## Files

- **`sync_worker.py`** - Automated sync worker for cron (NEW - 2026-01-28)
- `migrate_sqlite_to_postgres.py` - Migrate data from SQLite to PostgreSQL
- `MIGRATION_GUIDE.md` - Complete migration documentation
- `requirements.txt` - Python dependencies

---

## üîÑ Sync Worker (Production)

### Overview

`sync_worker.py` - Standalone async worker for automated data synchronization via cron.

**Pipeline Phases:**
1. **Collection** - Fetch latest article metrics from DEV.to API
2. **Enrichment** - Sync 90-day historical analytics  
3. **Intelligence** - Classify articles by theme (DNA analysis)
4. **Refresh Cache** - Update materialized views

### Features

‚úÖ **Asynchronous execution** - Efficient I/O with AsyncIO  
‚úÖ **PostgreSQL advisory locks** - Prevents concurrent runs  
‚úÖ **Idempotent operations** - Safe to run multiple times  
‚úÖ **Cron-friendly** - Proper exit codes for monitoring  
‚úÖ **Timestamped logging** - Easy debugging  

### Quick Start

```bash
# Manual execution
python3 scripts/sync_worker.py

# Expected output:
# [2026-01-28 09:30:00] üöÄ Starting Sync Pipeline...
# [2026-01-28 09:30:15] ‚úÖ Collection: 25 articles updated in 13.2s
# [2026-01-28 09:30:45] ‚úÖ Enrichment: 2250 snapshots synced in 30.1s
# [2026-01-28 09:30:50] ‚úÖ Intelligence: 25 articles classified in 5.3s
# [2026-01-28 09:30:51] ‚úÖ Cache refresh completed in 0.8s
# [2026-01-28 09:30:51] üéâ Pipeline completed successfully in 49.4s
```

### Cron Configuration

```bash
# Edit crontab
crontab -e

# Hourly sync at minute 5
5 * * * * cd /path/to/devto_githubcopilotcli_challenge && python3 scripts/sync_worker.py >> logs/sync_worker.log 2>&1

# Or every 6 hours (balanced - recommended)
5 0,6,12,18 * * * cd /path/to/devto_githubcopilotcli_challenge && python3 scripts/sync_worker.py >> logs/sync_worker.log 2>&1

# Or daily at 2 AM (low frequency)
5 2 * * * cd /path/to/devto_githubcopilotcli_challenge && python3 scripts/sync_worker.py >> logs/sync_worker.log 2>&1
```

### Environment Variables

Loads from `.env` in project root:

```env
# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=devto_analytics
POSTGRES_USER=devto
POSTGRES_PASSWORD=your_secure_password

# DEV.to API
DEVTO_API_KEY=your_devto_api_key
DEVTO_USERNAME=your_username
```

### Monitoring

```bash
# View live logs
tail -f logs/sync_worker.log

# Check for errors
grep "ERROR" logs/sync_worker.log

# Count successful runs today
grep "Pipeline completed successfully" logs/sync_worker.log | grep "$(date +%Y-%m-%d)" | wc -l

# View summary stats
grep "SYNC SUMMARY" logs/sync_worker.log -A 5
```

### Exit Codes

- **0** - Success or lock unavailable (normal, safe for cron)
- **1** - Error occurred (check logs for stack trace)

### Advisory Lock

Uses PostgreSQL advisory lock (ID: `98765`) to prevent concurrent execution.

**Check active locks:**
```sql
SELECT * FROM pg_locks WHERE locktype = 'advisory';
```

**Force release (emergency only):**
```sql
SELECT pg_advisory_unlock(98765);
```

### Performance

Expected durations for ~25 articles:
- Collection: 10-20 seconds
- Enrichment: 30-60 seconds (90 days)
- Intelligence: 5-10 seconds
- Cache Refresh: 1-3 seconds
- **Total: 50-90 seconds**

### Troubleshooting

**Lock always held:**
```sql
-- Release manually
SELECT pg_advisory_unlock(98765);
```

**Import errors:**
```bash
# Run from project root
cd /path/to/devto_githubcopilotcli_challenge
python3 scripts/sync_worker.py
```

**API rate limiting:**
- Reduce cron frequency (e.g., every 6 hours instead of hourly)
- Check API key: `curl -H "api-key: YOUR_KEY" https://dev.to/api/articles/me/all`

---

## üóÑÔ∏è Migration Scripts

### `migrate_sqlite_to_postgres.py`

Main migration script that transfers data from `devto_metrics.db` (SQLite) to PostgreSQL 18.

**Features:**
- ‚úÖ Schema-aware using `app/db/tables.py`
- ‚úÖ Batch processing (1,000 rows per batch)
- ‚úÖ Progress bars with tqdm
- ‚úÖ Data transformations (tags, dates, JSONB)
- ‚úÖ ON CONFLICT handling for idempotency
- ‚úÖ Dependency-aware table ordering
- ‚úÖ Comprehensive error handling

**Usage:**
```bash
python3 scripts/migrate_sqlite_to_postgres.py
```

See [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md) for complete documentation.

## Installation

```bash
# Install dependencies
pip install -r scripts/requirements.txt

# Or if you already installed app/requirements.txt, just add:
pip install tqdm python-dateutil
```

## Prerequisites

1. **Initialize PostgreSQL schema:**
   ```bash
   cd app
   python3 init_database.py
   ```

2. **Configure environment** (`.env`):
   ```bash
   POSTGRES_HOST=localhost
   POSTGRES_PORT=5432
   POSTGRES_USER=devto
   POSTGRES_PASSWORD=your_password
   POSTGRES_DB=devto_analytics
   ```

3. **Ensure SQLite database exists:**
   ```bash
   ls -lh devto_metrics.db
   ```

## Quick Start

```bash
# 1. Check you're in project root
pwd
# Should show: /path/to/devto_githubcopilotcli_challenge

# 2. Run migration
python3 scripts/migrate_sqlite_to_postgres.py

# 3. Verify
psql devto_analytics -c "SELECT * FROM devto_analytics.get_table_sizes();"
```

## What Gets Migrated

All tables from SQLite schema:
- ‚úÖ snapshots
- ‚úÖ article_metrics
- ‚úÖ comments
- ‚úÖ followers
- ‚úÖ follower_events
- ‚úÖ daily_analytics
- ‚úÖ referrers
- ‚úÖ article_content
- ‚úÖ article_code_blocks
- ‚úÖ article_links
- ‚úÖ comment_insights
- ‚úÖ article_history
- ‚úÖ milestone_events
- ‚úÖ author_themes
- ‚úÖ article_theme_mapping
- ‚úÖ article_stats_cache
- ‚úÖ tag_evolution
- ‚úÖ quality_scores

## Data Transformations

### Tags
```python
# SQLite ‚Üí PostgreSQL
'["python", "sql"]' ‚Üí ['python', 'sql']  # ARRAY(String)
'python,sql'        ‚Üí ['python', 'sql']
```

### Dates
```python
# All dates ‚Üí timezone-aware UTC
'2024-01-23T12:34:56Z' ‚Üí datetime(2024, 1, 23, 12, 34, 56, tzinfo=UTC)
```

### JSONB
```python
# JSON strings ‚Üí Python dicts
'{"key": "value"}' ‚Üí {'key': 'value'}
```

## Safety

The migration is **idempotent** (safe to run multiple times):
- Uses `INSERT ... ON CONFLICT DO NOTHING`
- Existing data is preserved
- Only new rows are added
- No duplicates created

## Performance

Typical speed: **500-1000 rows/second**

| Dataset Size | Expected Time |
|--------------|---------------|
| 1,000 rows | 1-2 seconds |
| 10,000 rows | 10-20 seconds |
| 100,000 rows | 2-3 minutes |

## Troubleshooting

### Common Issues

**"Schema not found"**
```bash
python3 app/init_database.py
```

**"Connection refused"**
```bash
# Check PostgreSQL is running
docker-compose ps postgres
# or
systemctl status postgresql
```

**"Module not found"**
```bash
pip install -r scripts/requirements.txt
```

See [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md) for detailed troubleshooting.

## Files

```
scripts/
‚îú‚îÄ‚îÄ README.md                      # This file
‚îú‚îÄ‚îÄ MIGRATION_GUIDE.md             # Complete documentation
‚îú‚îÄ‚îÄ migrate_sqlite_to_postgres.py  # Main migration script
‚îî‚îÄ‚îÄ requirements.txt               # Python dependencies
```

## Next Steps

After migration:
1. Verify data integrity
2. Run analytics queries
3. Test business logic
4. Set up continuous collection

See [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md) for detailed next steps.

---

**Status**: Production-ready  
**Safety**: Idempotent, preserves data  
**Performance**: ~500-1000 rows/s
